{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import scipy.io \n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import keras\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import cv2\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "# from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# import keras_metrics\n",
    "\n",
    "from keras.applications import mobilenet, resnet50 #, vgg16, inception_v3, resnet50, \n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, History\n",
    "\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "\n",
    "import logging\n",
    "# logging.getLogger().setLevel(logging.DEBUG)\n",
    "\n",
    "\n",
    "import seaborn\n",
    "seaborn.set_style(\"darkgrid\")\n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.metrics import precision_recall_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.5'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.__version__"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.4\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_dir = 'E:\\\\Work/PathoBarIlan/Shlomi2018/'# '/media/leetwito/DATA/Datasets/PathoBarIlan/Shlomi2018'\n",
    "is_relative_path_csv = False\n",
    "seed = 4221\n",
    "\n",
    "pos_name_init = 'Cancer'\n",
    "neg_name_init = 'Normal'\n",
    "\n",
    "use_rgb = False # True=rgb, False=spectral\n",
    "if use_rgb:\n",
    "    file_ext = '.png'\n",
    "else:\n",
    "    file_ext = '.npy'\n",
    "    \n",
    "window_size = (200, 200)\n",
    "shift = (100, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "w,h = window_size\n",
    "if use_rgb:\n",
    "    input_shape = (w,h,3)\n",
    "else:\n",
    "    input_shape = (w,h,40)\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_slide(path):\n",
    "    mat = scipy.io.loadmat(path)\n",
    "    spectral = mat[\"Spec\"]\n",
    "    rgb = mat[\"Section\"]\n",
    "    shape = rgb.shape\n",
    "    \n",
    "    return spectral, rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batch_of_crops_from_slide(img, window_size, shift, vis_flag=False):\n",
    "    crops = []\n",
    "\n",
    "    n_iter_x = (img.shape[1]-window_size[0])//shift[0] + 1\n",
    "\n",
    "    n_iter_y = (img.shape[0]-window_size[1])//shift[1] + 1\n",
    "\n",
    "#     n_iter_x, n_iter_y\n",
    "\n",
    "    for i in range(n_iter_x):\n",
    "        for j in range(n_iter_y):\n",
    "            init_y = i*shift[0]\n",
    "            init_x = j*shift[1]\n",
    "        \n",
    "            crops.append(img[init_x:init_x+window_size[0], init_y:init_y+window_size[1], :])\n",
    "    if vis_flag:\n",
    "        visualize_batch_of_crops(crops, n_iter_y, n_iter_x)\n",
    "    return crops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_batch_of_crops(crops, n_iter_y, n_iter_x):\n",
    "    fig, axes = plt.subplots(n_iter_y, n_iter_x, figsize=(5, 5), gridspec_kw = {'wspace':0, 'hspace':0})\n",
    "\n",
    "    for i in range(n_iter_x):\n",
    "        for j in range(n_iter_y):\n",
    "            axes[j, i].imshow(crops[i*n_iter_y + j])\n",
    "            axes[j, i].axis('off')\n",
    "            axes[j, i].set_aspect('equal')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_crops_from_fileslist(fileslist, window_size, shift):\n",
    "    rgb_crops = []\n",
    "    spectral_crops = []\n",
    "    labels = []\n",
    "\n",
    "    for file in tqdm(fileslist):\n",
    "#         file_name = os.path.basename(file)\n",
    "#         print('Saving crops for file {} ...'.format(file_name))\n",
    "#         print(file)\n",
    "        spectral, rgb = read_slide(file)\n",
    "        spectral_crops = create_batch_of_crops_from_slide(spectral, window_size=window_size, shift=shift)\n",
    "        rgb_crops = create_batch_of_crops_from_slide(rgb, window_size=window_size, shift=shift)\n",
    "        save_dir = file.replace('.mat', '_win{}-{}_shift{}-{}'.format(window_size[0], window_size[1], shift[0], shift[1]))\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "        for idx, (im_np, spec_np) in enumerate(zip(rgb_crops, spectral_crops)):\n",
    "            im = Image.fromarray(im_np)\n",
    "            im.save(os.path.join(save_dir, '{:05}.png'.format(idx)))\n",
    "            np.save(os.path.join(save_dir, '{:05}.npy'.format(idx)), spec_np)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_crops_from_dir(dir_path, window_size, shift):\n",
    "    print('Saving crops for slides in dir: {}'.format(dir_path))\n",
    "    fileslist = glob(dir_path + '/*.mat')\n",
    "    create_crops_from_fileslist(fileslist, window_size, shift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_csv_for_folder(data_dir, ext):\n",
    "    if ext[0] == '.':\n",
    "        ext = ext[1:]\n",
    "    data_df = pd.DataFrame(columns=['filename', 'label'])\n",
    "    files = glob(os.path.join(data_dir,'*', '*.{}'.format(ext)))\n",
    "    files = [file for file in files if \"Mixed\" not in file]\n",
    "#     print(data_dir+'/*/*.{}'.format(ext))\n",
    "    \n",
    "    init_len = len(data_dir)\n",
    "    delete_folder = all_data_dir\n",
    "    if not is_relative_path_csv:\n",
    "        delete_folder = '/'\n",
    "    if not delete_folder[-1] == '/':\n",
    "        delete_folder += '/'\n",
    "    files = [file.replace(delete_folder, '/') for file in files]\n",
    "#     print(files)\n",
    "    labels = [1 if pos_name_init in file else 0 for file in files]\n",
    "#     print(labels)\n",
    "    data_df['filename'] = files\n",
    "    data_df['label'] = labels\n",
    "#     data_df.to_csv(os.path.join(data_dir, os.path.basename(data_dir)+'.csv'), index=False)\n",
    "#     print('Created CSV successfully for folder {}'.format(data_dir))\n",
    "    \n",
    "    return data_df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['E:\\\\Work/PathoBarIlan/Shlomi2018\\\\Case10\\\\',\n",
       " 'E:\\\\Work/PathoBarIlan/Shlomi2018\\\\Case11\\\\',\n",
       " 'E:\\\\Work/PathoBarIlan/Shlomi2018\\\\Case12\\\\',\n",
       " 'E:\\\\Work/PathoBarIlan/Shlomi2018\\\\Case14\\\\',\n",
       " 'E:\\\\Work/PathoBarIlan/Shlomi2018\\\\Case16\\\\',\n",
       " 'E:\\\\Work/PathoBarIlan/Shlomi2018\\\\Case16b\\\\',\n",
       " 'E:\\\\Work/PathoBarIlan/Shlomi2018\\\\Case17\\\\',\n",
       " 'E:\\\\Work/PathoBarIlan/Shlomi2018\\\\Case18\\\\',\n",
       " 'E:\\\\Work/PathoBarIlan/Shlomi2018\\\\Case19484\\\\',\n",
       " 'E:\\\\Work/PathoBarIlan/Shlomi2018\\\\Case8\\\\']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slides = glob(os.path.join(all_data_dir, \"*/\"))\n",
    "slides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [2 3 4 5 6 7 8 9] TEST: [0 1]\n",
      "TRAIN: [0 1 2 3 5 7 8 9] TEST: [4 6]\n",
      "TRAIN: [0 1 3 4 5 6 7 8] TEST: [2 9]\n",
      "TRAIN: [0 1 2 4 6 7 8 9] TEST: [3 5]\n",
      "TRAIN: [0 1 2 3 4 5 6 9] TEST: [7 8]\n"
     ]
    }
   ],
   "source": [
    "skf = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "\n",
    "train_slides_all = []\n",
    "test_slides_all = []\n",
    "val_slides_all = []\n",
    "\n",
    "for train_index, test_index in skf.split(np.arange(len(slides)).T, np.arange(len(slides)).T):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    train_slides_all.append(train_index)\n",
    "    val_slides_all.append([test_index[0]])\n",
    "    test_slides_all.append([test_index[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 4, 6, 7, 8, 9]), [3], [5])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 3 # take one of the K-Folds\n",
    "\n",
    "train_index = train_slides_all[i]\n",
    "val_index = val_slides_all[i]\n",
    "test_index = test_slides_all[i]\n",
    "\n",
    "train_index, val_index, test_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dfs_for_indices(slides, index_list):\n",
    "    dfs = []\n",
    "    for slide in np.array(slides)[index_list]:\n",
    "        data_dir = slide\n",
    "        dfs.append(create_csv_for_folder(data_dir, file_ext))\n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "    df = df.sample(frac=1, random_state=seed)  # frac=1 is same as shuffling df.\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = get_dfs_for_indices(slides, train_index)\n",
    "df_test = get_dfs_for_indices(slides, test_index)\n",
    "df_val = get_dfs_for_indices(slides, val_index)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_train.to_csv(os.path.join(all_data_dir, 'train_files.csv'), index=False)\n",
    "df_val.to_csv(os.path.join(all_data_dir, 'val_files.csv'), index=False)\n",
    "df_test.to_csv(os.path.join(all_data_dir, 'test_files.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2991\n",
      "2991\n",
      "2\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(len(df_train.index.values))\n",
    "print(len(set(df_train.index.values)))\n",
    "\n",
    "print(len(df_train.columns.values))\n",
    "print(len(set(df_train.columns.values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(set(df_train.label.values)) == 2 and len(set(df_val.label.values)) == 2 and len(set(df_test.label.values)) == 2  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_train = df_train[:35]\n",
    "df_test = df_train\n",
    "df_val = df_train"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "n_batches_train = df_train.shape[0]//batch_size\n",
    "n_batches_test = df_test.shape[0]//batch_size\n",
    "n_batches_val = df_val.shape[0]//batch_size"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df = df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_norm(x):\n",
    "#     print(\"x.shape:\", x.shape)\n",
    "    maxi = x.max(axis=1).max(axis=1)\n",
    "#     print(\"maxi.shape:\", maxi.shape)\n",
    "    maxi = np.repeat(maxi[:, np.newaxis, : ], window_size[0], axis=1)\n",
    "    maxi = np.repeat(maxi[:, np.newaxis, : ], window_size[1], axis=1)\n",
    "    return x/maxi\n",
    "\n",
    "\n",
    "def generator_from_df(df, batch_size, shuffle=True): \n",
    "    \n",
    "    n_batches = df.shape[0]//batch_size\n",
    "    while True:\n",
    "        if shuffle:\n",
    "            df_tmp = df.copy().sample(frac=1)  # frac=1 is same as shuffling df.\n",
    "        else:\n",
    "            df_tmp = df\n",
    "        \n",
    "        for i in range(n_batches):\n",
    "            sub = df_tmp.iloc[batch_size*i:batch_size*(i+1)]\n",
    "            if use_rgb:\n",
    "                X = [img_to_array(load_img(f, target_size=input_shape)) for f in sub.filename]\n",
    "            else:\n",
    "                X = [np.load(f) for f in sub.filename]\n",
    "                \n",
    "            X = sample_norm(np.stack(X))\n",
    "            logging.debug(f\"from file {sub.iloc[0].filename}\\nto file {sub.iloc[-1].filename}\")\n",
    "\n",
    "            Y = sub.label.values\n",
    "            Y = to_categorical(Y, num_classes=2)\n",
    "            # Simple model, one input, one output.\n",
    "            \n",
    "            yield X, Y"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_generator = generator_from_df(df_train.iloc[:42], batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df.filename.reset_index(drop=True)[:42]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X, y = next(train_generator)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####### copying generator_from_df:\n",
    "https://gist.github.com/timehaven/257eef5b0e2d9e2625a9eb812ca2226b#file-akmtdfgen-py"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def sample_norm(X):\n",
    "#     X = X - X.min()\n",
    "#     X = X / X.max()\n",
    "#     X = X - 0.5\n",
    "    # print(X.min(), X.max()) -> (-0.5, 0.5)\n",
    "    \n",
    "#     X = X / 255.\n",
    "    return X\n",
    "\n",
    "def generator_from_df(df, batch_size, shuffle=True): \n",
    "#     print(df.iloc[1])\n",
    "    nbatches, n_skipped_per_epoch = divmod(df.shape[0], batch_size)\n",
    "    count = 1\n",
    "    epoch = 0\n",
    "    while 1:\n",
    "        if shuffle:\n",
    "            df = df.sample(frac=1)  # frac=1 is same as shuffling df.\n",
    "        epoch += 1\n",
    "        i, j = 0, batch_size\n",
    "        # Mini-batches within epoch.\n",
    "        mini_batches_completed = 0\n",
    "        for _ in range(nbatches):\n",
    "#             print(\"Top of generator for loop, epoch / count / i / j = %d / %d / %d / %d\" % (epoch, count, i, j))\n",
    "            sub = df.iloc[i:j]\n",
    "            # preprocess_input()\n",
    "            # https://github.com/fchollet/keras/blob/master/keras/applications/inception_v3.py#L389\n",
    "            if use_rgb:\n",
    "                X = [sample_norm(img_to_array(load_img(f, target_size=input_shape))) for f in sub.filename]\n",
    "            else:\n",
    "                X = [sample_norm(np.load(f)) for f in sub.filename]\n",
    "                     # Resizing on the fly is efficient enough for\n",
    "                     # pre-caching when a GPU is training a\n",
    "                     # mini-batch.  Here is where some additional\n",
    "                     # data augmentation could take place.\n",
    "#                          (img_to_array(load_img(f, target_size=target_size))\n",
    "            X = np.stack(X)\n",
    "            Y = sub.label.values\n",
    "            Y = to_categorical(Y, num_classes=2)\n",
    "            # Simple model, one input, one output.\n",
    "            mini_batches_completed += 1\n",
    "            i = j \n",
    "            j += batch_size\n",
    "            yield X, Y"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_train = df_train[:batch_size]\n",
    "df_val = df_val[:batch_size]\n",
    "df_test = df_test[:batch_size]\n",
    "\n",
    "# df_val = df_train[:batch_size]\n",
    "# df_test = df_train[:batch_size]\n",
    "# assert(df_val == df_train).values.all()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_val = df_train\n",
    "df_test = df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator_from_df(df_train, batch_size)\n",
    "val_generator = generator_from_df(df_val, batch_size)\n",
    "test_generator = generator_from_df(df_test, batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for xx, yy in train_generator:\n",
    "    print(xx.shape, \"\\n\", yy)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\envs\\keras\\lib\\site-packages\\keras\\applications\\imagenet_utils.py:258: UserWarning: This model usually expects 1 or 3 input channels. However, it was passed an input_shape with 40 input channels.\n",
      "  str(input_shape[-1]) + ' input channels.')\n"
     ]
    }
   ],
   "source": [
    "# input_shape = train_generator.image_shape\n",
    "mobilenet_model = mobilenet.MobileNet(include_top=True, weights=None, input_shape=input_shape, classes=2, dropout=0.2)\n",
    "# mobilenet_model = resnet50.ResNet50(include_top=True, weights=None, input_shape=input_shape, classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 200, 200, 40)      0         \n",
      "_________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)    (None, 202, 202, 40)      0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 100, 100, 32)      11520     \n",
      "_________________________________________________________________\n",
      "conv1_bn (BatchNormalization (None, 100, 100, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv1_relu (Activation)      (None, 100, 100, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv_pad_1 (ZeroPadding2D)   (None, 102, 102, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv_dw_1 (DepthwiseConv2D)  (None, 100, 100, 32)      288       \n",
      "_________________________________________________________________\n",
      "conv_dw_1_bn (BatchNormaliza (None, 100, 100, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv_dw_1_relu (Activation)  (None, 100, 100, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv_pw_1 (Conv2D)           (None, 100, 100, 64)      2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_1_bn (BatchNormaliza (None, 100, 100, 64)      256       \n",
      "_________________________________________________________________\n",
      "conv_pw_1_relu (Activation)  (None, 100, 100, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv_pad_2 (ZeroPadding2D)   (None, 102, 102, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv_dw_2 (DepthwiseConv2D)  (None, 50, 50, 64)        576       \n",
      "_________________________________________________________________\n",
      "conv_dw_2_bn (BatchNormaliza (None, 50, 50, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv_dw_2_relu (Activation)  (None, 50, 50, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_2 (Conv2D)           (None, 50, 50, 128)       8192      \n",
      "_________________________________________________________________\n",
      "conv_pw_2_bn (BatchNormaliza (None, 50, 50, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_pw_2_relu (Activation)  (None, 50, 50, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_pad_3 (ZeroPadding2D)   (None, 52, 52, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_3 (DepthwiseConv2D)  (None, 50, 50, 128)       1152      \n",
      "_________________________________________________________________\n",
      "conv_dw_3_bn (BatchNormaliza (None, 50, 50, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_dw_3_relu (Activation)  (None, 50, 50, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_3 (Conv2D)           (None, 50, 50, 128)       16384     \n",
      "_________________________________________________________________\n",
      "conv_pw_3_bn (BatchNormaliza (None, 50, 50, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_pw_3_relu (Activation)  (None, 50, 50, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_pad_4 (ZeroPadding2D)   (None, 52, 52, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_4 (DepthwiseConv2D)  (None, 25, 25, 128)       1152      \n",
      "_________________________________________________________________\n",
      "conv_dw_4_bn (BatchNormaliza (None, 25, 25, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_dw_4_relu (Activation)  (None, 25, 25, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_4 (Conv2D)           (None, 25, 25, 256)       32768     \n",
      "_________________________________________________________________\n",
      "conv_pw_4_bn (BatchNormaliza (None, 25, 25, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_pw_4_relu (Activation)  (None, 25, 25, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_pad_5 (ZeroPadding2D)   (None, 27, 27, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_5 (DepthwiseConv2D)  (None, 25, 25, 256)       2304      \n",
      "_________________________________________________________________\n",
      "conv_dw_5_bn (BatchNormaliza (None, 25, 25, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_dw_5_relu (Activation)  (None, 25, 25, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_5 (Conv2D)           (None, 25, 25, 256)       65536     \n",
      "_________________________________________________________________\n",
      "conv_pw_5_bn (BatchNormaliza (None, 25, 25, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_pw_5_relu (Activation)  (None, 25, 25, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_pad_6 (ZeroPadding2D)   (None, 27, 27, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_6 (DepthwiseConv2D)  (None, 13, 13, 256)       2304      \n",
      "_________________________________________________________________\n",
      "conv_dw_6_bn (BatchNormaliza (None, 13, 13, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_dw_6_relu (Activation)  (None, 13, 13, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_6 (Conv2D)           (None, 13, 13, 512)       131072    \n",
      "_________________________________________________________________\n",
      "conv_pw_6_bn (BatchNormaliza (None, 13, 13, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_6_relu (Activation)  (None, 13, 13, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pad_7 (ZeroPadding2D)   (None, 15, 15, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_7 (DepthwiseConv2D)  (None, 13, 13, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_7_bn (BatchNormaliza (None, 13, 13, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_7_relu (Activation)  (None, 13, 13, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_7 (Conv2D)           (None, 13, 13, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_7_bn (BatchNormaliza (None, 13, 13, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_7_relu (Activation)  (None, 13, 13, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pad_8 (ZeroPadding2D)   (None, 15, 15, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_8 (DepthwiseConv2D)  (None, 13, 13, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_8_bn (BatchNormaliza (None, 13, 13, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_8_relu (Activation)  (None, 13, 13, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_8 (Conv2D)           (None, 13, 13, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_8_bn (BatchNormaliza (None, 13, 13, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_8_relu (Activation)  (None, 13, 13, 512)       0         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "conv_pad_9 (ZeroPadding2D)   (None, 15, 15, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_9 (DepthwiseConv2D)  (None, 13, 13, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_9_bn (BatchNormaliza (None, 13, 13, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_9_relu (Activation)  (None, 13, 13, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_9 (Conv2D)           (None, 13, 13, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_9_bn (BatchNormaliza (None, 13, 13, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_9_relu (Activation)  (None, 13, 13, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pad_10 (ZeroPadding2D)  (None, 15, 15, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_10 (DepthwiseConv2D) (None, 13, 13, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_10_bn (BatchNormaliz (None, 13, 13, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_10_relu (Activation) (None, 13, 13, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_10 (Conv2D)          (None, 13, 13, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_10_bn (BatchNormaliz (None, 13, 13, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_10_relu (Activation) (None, 13, 13, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pad_11 (ZeroPadding2D)  (None, 15, 15, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_11 (DepthwiseConv2D) (None, 13, 13, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_11_bn (BatchNormaliz (None, 13, 13, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_11_relu (Activation) (None, 13, 13, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_11 (Conv2D)          (None, 13, 13, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_11_bn (BatchNormaliz (None, 13, 13, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_11_relu (Activation) (None, 13, 13, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pad_12 (ZeroPadding2D)  (None, 15, 15, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_12 (DepthwiseConv2D) (None, 7, 7, 512)         4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_12_bn (BatchNormaliz (None, 7, 7, 512)         2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_12_relu (Activation) (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv_pw_12 (Conv2D)          (None, 7, 7, 1024)        524288    \n",
      "_________________________________________________________________\n",
      "conv_pw_12_bn (BatchNormaliz (None, 7, 7, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_pw_12_relu (Activation) (None, 7, 7, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv_pad_13 (ZeroPadding2D)  (None, 9, 9, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv_dw_13 (DepthwiseConv2D) (None, 7, 7, 1024)        9216      \n",
      "_________________________________________________________________\n",
      "conv_dw_13_bn (BatchNormaliz (None, 7, 7, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_dw_13_relu (Activation) (None, 7, 7, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_13 (Conv2D)          (None, 7, 7, 1024)        1048576   \n",
      "_________________________________________________________________\n",
      "conv_pw_13_bn (BatchNormaliz (None, 7, 7, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_pw_13_relu (Activation) (None, 7, 7, 1024)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 1, 1, 1024)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1, 1, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv_preds (Conv2D)          (None, 1, 1, 2)           2050      \n",
      "_________________________________________________________________\n",
      "act_softmax (Activation)     (None, 1, 1, 2)           0         \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 3,241,570\n",
      "Trainable params: 3,219,682\n",
      "Non-trainable params: 21,888\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mobilenet_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(lr=1e-2) # 1e-3\n",
    "mobilenet_model.compile(loss=\"binary_crossentropy\", optimizer=optimizer) #  binary_crossentropy , categorical_crossentropy\n",
    "# history = History()\n",
    "lrReduce = ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=4, verbose=1, min_lr=1e-6)\n",
    "if use_rgb:\n",
    "    chkpnt = ModelCheckpoint(\"my_models/model_rgb_weights_epoch{epoch:02d}-val_loss{val_loss:.3f}.hdf5\", save_best_only=True) # -train_loss{history.History()[loss][-1]:.2f}\n",
    "else:\n",
    "    chkpnt = ModelCheckpoint(\"my_models/model_spec_weights_epoch{epoch:02d}-val_loss{val_loss:.3f}.hdf5\", save_best_only=True) # -train_loss{history.History()[loss][-1]:.2f}\n",
    "num_of_epochs = 100"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# continue training\n",
    "STEP_SIZE_TRAIN=len(df_train)//batch_size\n",
    "STEP_SIZE_VALID=len(df_val)//batch_size\n",
    "\n",
    "# pretrained_model_path = '/media/leetwito/Windows/Users/leetw/PycharmProjects/PathoBarIlan/my_models/model_spec_weights_epoch15-val_loss0.001.hdf5'\n",
    "pretrained_model_path = all_data_dir + \"models/model_spec_weights_epoch15-val_loss0.001.hdf5'\"\n",
    "# loaded_model = keras.models.load_model(pretrained_model_path)\n",
    "history = loaded_model.fit_generator(generator=train_generator,\n",
    "                    steps_per_epoch=STEP_SIZE_TRAIN,\n",
    "                    validation_data=val_generator,\n",
    "                    validation_steps=STEP_SIZE_VALID,\n",
    "                    epochs=num_of_epochs, callbacks=[lrReduce, chkpnt], shuffle=False) # chkpnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "186/186 [==============================] - ETA: 28:47 - loss: 0.76 - ETA: 15:13 - loss: 1.20 - ETA: 10:41 - loss: 1.05 - ETA: 8:25 - loss: 0.7950 - ETA: 8:17 - loss: 1.056 - ETA: 8:16 - loss: 0.994 - ETA: 8:14 - loss: 0.973 - ETA: 8:12 - loss: 0.910 - ETA: 8:06 - loss: 0.846 - ETA: 8:02 - loss: 0.788 - ETA: 7:58 - loss: 0.732 - ETA: 7:53 - loss: 0.685 - ETA: 7:52 - loss: 0.634 - ETA: 7:49 - loss: 0.619 - ETA: 7:49 - loss: 0.601 - ETA: 7:44 - loss: 0.565 - ETA: 7:43 - loss: 0.551 - ETA: 7:39 - loss: 0.584 - ETA: 7:36 - loss: 0.567 - ETA: 7:32 - loss: 0.540 - ETA: 7:30 - loss: 0.536 - ETA: 7:28 - loss: 0.525 - ETA: 7:24 - loss: 0.554 - ETA: 7:23 - loss: 0.571 - ETA: 7:21 - loss: 0.564 - ETA: 7:18 - loss: 0.556 - ETA: 7:16 - loss: 0.545 - ETA: 7:14 - loss: 0.529 - ETA: 7:12 - loss: 0.527 - ETA: 7:10 - loss: 0.518 - ETA: 7:06 - loss: 0.504 - ETA: 7:04 - loss: 0.493 - ETA: 7:02 - loss: 0.490 - ETA: 7:00 - loss: 0.480 - ETA: 6:58 - loss: 0.470 - ETA: 6:55 - loss: 0.459 - ETA: 6:53 - loss: 0.449 - ETA: 6:51 - loss: 0.440 - ETA: 6:49 - loss: 0.432 - ETA: 6:47 - loss: 0.423 - ETA: 6:45 - loss: 0.418 - ETA: 6:42 - loss: 0.409 - ETA: 6:40 - loss: 0.399 - ETA: 6:38 - loss: 0.391 - ETA: 6:35 - loss: 0.384 - ETA: 6:33 - loss: 0.376 - ETA: 6:31 - loss: 0.369 - ETA: 6:28 - loss: 0.362 - ETA: 6:26 - loss: 0.359 - ETA: 6:24 - loss: 0.353 - ETA: 6:21 - loss: 0.346 - ETA: 6:19 - loss: 0.340 - ETA: 6:17 - loss: 0.333 - ETA: 6:14 - loss: 0.330 - ETA: 6:11 - loss: 0.331 - ETA: 6:08 - loss: 0.329 - ETA: 6:06 - loss: 0.325 - ETA: 6:04 - loss: 0.320 - ETA: 6:01 - loss: 0.316 - ETA: 5:58 - loss: 0.312 - ETA: 5:56 - loss: 0.327 - ETA: 5:53 - loss: 0.325 - ETA: 5:50 - loss: 0.325 - ETA: 5:48 - loss: 0.321 - ETA: 5:45 - loss: 0.324 - ETA: 5:42 - loss: 0.322 - ETA: 5:40 - loss: 0.321 - ETA: 5:37 - loss: 0.321 - ETA: 5:34 - loss: 0.320 - ETA: 5:31 - loss: 0.318 - ETA: 5:29 - loss: 0.315 - ETA: 5:26 - loss: 0.311 - ETA: 5:23 - loss: 0.309 - ETA: 5:21 - loss: 0.306 - ETA: 5:18 - loss: 0.303 - ETA: 5:16 - loss: 0.301 - ETA: 5:13 - loss: 0.298 - ETA: 5:10 - loss: 0.296 - ETA: 5:07 - loss: 0.293 - ETA: 5:05 - loss: 0.291 - ETA: 5:02 - loss: 0.289 - ETA: 4:59 - loss: 0.286 - ETA: 4:56 - loss: 0.285 - ETA: 4:54 - loss: 0.284 - ETA: 4:51 - loss: 0.283 - ETA: 4:48 - loss: 0.281 - ETA: 4:45 - loss: 0.278 - ETA: 4:43 - loss: 0.276 - ETA: 4:40 - loss: 0.276 - ETA: 4:37 - loss: 0.273 - ETA: 4:34 - loss: 0.270 - ETA: 4:31 - loss: 0.268 - ETA: 4:28 - loss: 0.267 - ETA: 4:26 - loss: 0.265 - ETA: 4:23 - loss: 0.262 - ETA: 4:20 - loss: 0.260 - ETA: 4:17 - loss: 0.258 - ETA: 4:14 - loss: 0.255 - ETA: 4:11 - loss: 0.253 - ETA: 4:09 - loss: 0.267 - ETA: 4:06 - loss: 0.269 - ETA: 4:03 - loss: 0.267 - ETA: 4:00 - loss: 0.265 - ETA: 3:57 - loss: 0.265 - ETA: 3:55 - loss: 0.264 - ETA: 3:52 - loss: 0.262 - ETA: 3:49 - loss: 0.260 - ETA: 3:46 - loss: 0.258 - ETA: 3:43 - loss: 0.264 - ETA: 3:40 - loss: 0.262 - ETA: 3:37 - loss: 0.261 - ETA: 3:34 - loss: 0.259 - ETA: 3:32 - loss: 0.258 - ETA: 3:29 - loss: 0.256 - ETA: 3:26 - loss: 0.254 - ETA: 3:23 - loss: 0.253 - ETA: 3:20 - loss: 0.251 - ETA: 3:17 - loss: 0.249 - ETA: 3:14 - loss: 0.248 - ETA: 3:11 - loss: 0.247 - ETA: 3:08 - loss: 0.247 - ETA: 3:06 - loss: 0.245 - ETA: 3:03 - loss: 0.245 - ETA: 3:00 - loss: 0.244 - ETA: 2:57 - loss: 0.244 - ETA: 2:54 - loss: 0.243 - ETA: 2:51 - loss: 0.241 - ETA: 2:48 - loss: 0.240 - ETA: 2:45 - loss: 0.238 - ETA: 2:43 - loss: 0.237 - ETA: 2:40 - loss: 0.236 - ETA: 2:37 - loss: 0.235 - ETA: 2:34 - loss: 0.233 - ETA: 2:31 - loss: 0.232 - ETA: 2:28 - loss: 0.230 - ETA: 2:25 - loss: 0.229 - ETA: 2:22 - loss: 0.227 - ETA: 2:19 - loss: 0.226 - ETA: 2:17 - loss: 0.225 - ETA: 2:14 - loss: 0.223 - ETA: 2:11 - loss: 0.222 - ETA: 2:08 - loss: 0.220 - ETA: 2:05 - loss: 0.221 - ETA: 2:02 - loss: 0.220 - ETA: 1:59 - loss: 0.219 - ETA: 1:56 - loss: 0.217 - ETA: 1:53 - loss: 0.216 - ETA: 1:50 - loss: 0.215 - ETA: 1:48 - loss: 0.216 - ETA: 1:45 - loss: 0.215 - ETA: 1:42 - loss: 0.220 - ETA: 1:39 - loss: 0.218 - ETA: 1:36 - loss: 0.217 - ETA: 1:33 - loss: 0.216 - ETA: 1:30 - loss: 0.214 - ETA: 1:27 - loss: 0.217 - ETA: 1:24 - loss: 0.216 - ETA: 1:21 - loss: 0.215 - ETA: 1:18 - loss: 0.214 - ETA: 1:15 - loss: 0.214 - ETA: 1:13 - loss: 0.213 - ETA: 1:10 - loss: 0.212 - ETA: 1:07 - loss: 0.211 - ETA: 1:04 - loss: 0.210 - ETA: 1:01 - loss: 0.210 - ETA: 58s - loss: 0.209 - ETA: 55s - loss: 0.20 - ETA: 52s - loss: 0.20 - ETA: 49s - loss: 0.20 - ETA: 46s - loss: 0.20 - ETA: 43s - loss: 0.20 - ETA: 40s - loss: 0.20 - ETA: 38s - loss: 0.20 - ETA: 35s - loss: 0.20 - ETA: 32s - loss: 0.20 - ETA: 29s - loss: 0.20 - ETA: 26s - loss: 0.20 - ETA: 23s - loss: 0.20 - ETA: 20s - loss: 0.20 - ETA: 17s - loss: 0.20 - ETA: 14s - loss: 0.20 - ETA: 11s - loss: 0.20 - ETA: 8s - loss: 0.2023 - ETA: 5s - loss: 0.201 - ETA: 2s - loss: 0.203 - 603s 3s/step - loss: 0.2027 - val_loss: 5.6516\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186/186 [==============================] - ETA: 1:47 - loss: 0.309 - ETA: 1:46 - loss: 0.187 - ETA: 1:47 - loss: 0.135 - ETA: 1:46 - loss: 0.109 - ETA: 1:45 - loss: 0.126 - ETA: 1:45 - loss: 0.149 - ETA: 1:45 - loss: 0.136 - ETA: 1:44 - loss: 0.121 - ETA: 1:43 - loss: 0.113 - ETA: 1:43 - loss: 0.103 - ETA: 1:42 - loss: 0.096 - ETA: 1:41 - loss: 0.093 - ETA: 2:03 - loss: 0.105 - ETA: 2:25 - loss: 0.098 - ETA: 2:48 - loss: 0.095 - ETA: 3:08 - loss: 0.099 - ETA: 3:24 - loss: 0.094 - ETA: 3:40 - loss: 0.090 - ETA: 3:54 - loss: 0.096 - ETA: 4:03 - loss: 0.093 - ETA: 4:14 - loss: 0.089 - ETA: 4:23 - loss: 0.087 - ETA: 4:31 - loss: 0.083 - ETA: 4:37 - loss: 0.082 - ETA: 4:43 - loss: 0.082 - ETA: 4:49 - loss: 0.079 - ETA: 4:55 - loss: 0.077 - ETA: 5:00 - loss: 0.074 - ETA: 5:03 - loss: 0.072 - ETA: 5:07 - loss: 0.069 - ETA: 5:10 - loss: 0.067 - ETA: 5:12 - loss: 0.072 - ETA: 5:14 - loss: 0.070 - ETA: 5:16 - loss: 0.073 - ETA: 5:18 - loss: 0.071 - ETA: 5:19 - loss: 0.069 - ETA: 5:21 - loss: 0.067 - ETA: 5:22 - loss: 0.065 - ETA: 5:23 - loss: 0.068 - ETA: 5:23 - loss: 0.068 - ETA: 5:23 - loss: 0.067 - ETA: 5:24 - loss: 0.084 - ETA: 5:23 - loss: 0.094 - ETA: 5:24 - loss: 0.092 - ETA: 5:23 - loss: 0.090 - ETA: 5:23 - loss: 0.089 - ETA: 5:23 - loss: 0.087 - ETA: 5:22 - loss: 0.090 - ETA: 5:22 - loss: 0.089 - ETA: 5:21 - loss: 0.088 - ETA: 5:20 - loss: 0.086 - ETA: 5:19 - loss: 0.087 - ETA: 5:18 - loss: 0.087 - ETA: 5:18 - loss: 0.086 - ETA: 5:16 - loss: 0.085 - ETA: 5:15 - loss: 0.083 - ETA: 5:14 - loss: 0.082 - ETA: 5:13 - loss: 0.081 - ETA: 5:12 - loss: 0.080 - ETA: 5:11 - loss: 0.079 - ETA: 5:09 - loss: 0.078 - ETA: 5:07 - loss: 0.079 - ETA: 5:06 - loss: 0.078 - ETA: 5:04 - loss: 0.078 - ETA: 5:03 - loss: 0.078 - ETA: 5:01 - loss: 0.077 - ETA: 4:59 - loss: 0.088 - ETA: 4:57 - loss: 0.087 - ETA: 4:56 - loss: 0.086 - ETA: 4:54 - loss: 0.085 - ETA: 4:52 - loss: 0.084 - ETA: 4:50 - loss: 0.084 - ETA: 4:48 - loss: 0.083 - ETA: 4:46 - loss: 0.083 - ETA: 4:44 - loss: 0.084 - ETA: 4:42 - loss: 0.084 - ETA: 4:40 - loss: 0.085 - ETA: 4:38 - loss: 0.085 - ETA: 4:36 - loss: 0.084 - ETA: 4:34 - loss: 0.084 - ETA: 4:32 - loss: 0.084 - ETA: 4:30 - loss: 0.083 - ETA: 4:28 - loss: 0.083 - ETA: 4:25 - loss: 0.084 - ETA: 4:23 - loss: 0.084 - ETA: 4:21 - loss: 0.083 - ETA: 4:19 - loss: 0.082 - ETA: 4:17 - loss: 0.082 - ETA: 4:14 - loss: 0.085 - ETA: 4:12 - loss: 0.084 - ETA: 4:10 - loss: 0.084 - ETA: 4:07 - loss: 0.084 - ETA: 4:05 - loss: 0.084 - ETA: 4:03 - loss: 0.083 - ETA: 4:00 - loss: 0.083 - ETA: 3:58 - loss: 0.082 - ETA: 3:56 - loss: 0.081 - ETA: 3:54 - loss: 0.081 - ETA: 3:51 - loss: 0.080 - ETA: 3:49 - loss: 0.088 - ETA: 3:46 - loss: 0.087 - ETA: 3:44 - loss: 0.087 - ETA: 3:41 - loss: 0.087 - ETA: 3:39 - loss: 0.087 - ETA: 3:36 - loss: 0.086 - ETA: 3:34 - loss: 0.086 - ETA: 3:31 - loss: 0.085 - ETA: 3:29 - loss: 0.087 - ETA: 3:26 - loss: 0.086 - ETA: 3:24 - loss: 0.086 - ETA: 3:21 - loss: 0.085 - ETA: 3:19 - loss: 0.084 - ETA: 3:16 - loss: 0.084 - ETA: 3:14 - loss: 0.085 - ETA: 3:11 - loss: 0.085 - ETA: 3:09 - loss: 0.084 - ETA: 3:06 - loss: 0.084 - ETA: 3:04 - loss: 0.083 - ETA: 3:01 - loss: 0.083 - ETA: 2:59 - loss: 0.082 - ETA: 2:56 - loss: 0.082 - ETA: 2:53 - loss: 0.081 - ETA: 2:51 - loss: 0.082 - ETA: 2:48 - loss: 0.081 - ETA: 2:46 - loss: 0.081 - ETA: 2:43 - loss: 0.082 - ETA: 2:40 - loss: 0.082 - ETA: 2:38 - loss: 0.082 - ETA: 2:35 - loss: 0.081 - ETA: 2:32 - loss: 0.080 - ETA: 2:30 - loss: 0.080 - ETA: 2:27 - loss: 0.080 - ETA: 2:25 - loss: 0.079 - ETA: 2:22 - loss: 0.079 - ETA: 2:19 - loss: 0.080 - ETA: 2:17 - loss: 0.079 - ETA: 2:14 - loss: 0.079 - ETA: 2:11 - loss: 0.079 - ETA: 2:09 - loss: 0.079 - ETA: 2:06 - loss: 0.078 - ETA: 2:03 - loss: 0.078 - ETA: 2:01 - loss: 0.077 - ETA: 1:58 - loss: 0.077 - ETA: 1:55 - loss: 0.078 - ETA: 1:53 - loss: 0.077 - ETA: 1:50 - loss: 0.077 - ETA: 1:47 - loss: 0.077 - ETA: 1:44 - loss: 0.077 - ETA: 1:42 - loss: 0.077 - ETA: 1:39 - loss: 0.077 - ETA: 1:36 - loss: 0.076 - ETA: 1:34 - loss: 0.076 - ETA: 1:31 - loss: 0.075 - ETA: 1:28 - loss: 0.075 - ETA: 1:25 - loss: 0.076 - ETA: 1:23 - loss: 0.075 - ETA: 1:20 - loss: 0.075 - ETA: 1:17 - loss: 0.074 - ETA: 1:14 - loss: 0.074 - ETA: 1:12 - loss: 0.074 - ETA: 1:09 - loss: 0.073 - ETA: 1:06 - loss: 0.073 - ETA: 1:03 - loss: 0.073 - ETA: 1:01 - loss: 0.072 - ETA: 58s - loss: 0.072 - ETA: 55s - loss: 0.07 - ETA: 52s - loss: 0.07 - ETA: 50s - loss: 0.07 - ETA: 47s - loss: 0.07 - ETA: 44s - loss: 0.07 - ETA: 41s - loss: 0.07 - ETA: 39s - loss: 0.07 - ETA: 36s - loss: 0.07 - ETA: 33s - loss: 0.07 - ETA: 30s - loss: 0.07 - ETA: 27s - loss: 0.07 - ETA: 25s - loss: 0.07 - ETA: 22s - loss: 0.06 - ETA: 19s - loss: 0.06 - ETA: 16s - loss: 0.06 - ETA: 13s - loss: 0.06 - ETA: 11s - loss: 0.06 - ETA: 8s - loss: 0.0690 - ETA: 5s - loss: 0.069 - ETA: 2s - loss: 0.070 - 577s 3s/step - loss: 0.0698 - val_loss: 4.3359\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186/186 [==============================] - ETA: 1:45 - loss: 0.003 - ETA: 1:45 - loss: 0.016 - ETA: 1:45 - loss: 0.058 - ETA: 1:45 - loss: 0.050 - ETA: 1:44 - loss: 0.041 - ETA: 1:45 - loss: 0.034 - ETA: 1:44 - loss: 0.030 - ETA: 1:43 - loss: 0.047 - ETA: 1:43 - loss: 0.042 - ETA: 1:43 - loss: 0.060 - ETA: 1:42 - loss: 0.064 - ETA: 1:42 - loss: 0.065 - ETA: 2:02 - loss: 0.070 - ETA: 2:29 - loss: 0.067 - ETA: 2:51 - loss: 0.064 - ETA: 3:10 - loss: 0.060 - ETA: 3:25 - loss: 0.060 - ETA: 3:38 - loss: 0.057 - ETA: 3:52 - loss: 0.061 - ETA: 4:02 - loss: 0.058 - ETA: 4:12 - loss: 0.055 - ETA: 4:21 - loss: 0.053 - ETA: 4:31 - loss: 0.051 - ETA: 4:39 - loss: 0.049 - ETA: 4:44 - loss: 0.048 - ETA: 4:50 - loss: 0.056 - ETA: 4:54 - loss: 0.062 - ETA: 4:59 - loss: 0.060 - ETA: 5:03 - loss: 0.061 - ETA: 5:07 - loss: 0.062 - ETA: 5:10 - loss: 0.068 - ETA: 5:12 - loss: 0.067 - ETA: 5:15 - loss: 0.071 - ETA: 5:17 - loss: 0.088 - ETA: 5:19 - loss: 0.086 - ETA: 5:20 - loss: 0.110 - ETA: 5:21 - loss: 0.108 - ETA: 5:21 - loss: 0.109 - ETA: 5:22 - loss: 0.108 - ETA: 5:23 - loss: 0.106 - ETA: 5:23 - loss: 0.106 - ETA: 5:24 - loss: 0.136 - ETA: 5:25 - loss: 0.137 - ETA: 5:25 - loss: 0.142 - ETA: 5:25 - loss: 0.140 - ETA: 5:25 - loss: 0.140 - ETA: 5:24 - loss: 0.138 - ETA: 5:24 - loss: 0.147 - ETA: 5:23 - loss: 0.147 - ETA: 5:23 - loss: 0.145 - ETA: 5:22 - loss: 0.146 - ETA: 5:21 - loss: 0.148 - ETA: 5:20 - loss: 0.147 - ETA: 5:19 - loss: 0.145 - ETA: 5:18 - loss: 0.143 - ETA: 5:17 - loss: 0.148 - ETA: 5:15 - loss: 0.146 - ETA: 5:14 - loss: 0.144 - ETA: 5:12 - loss: 0.143 - ETA: 5:11 - loss: 0.145 - ETA: 5:09 - loss: 0.143 - ETA: 5:08 - loss: 0.147 - ETA: 5:07 - loss: 0.145 - ETA: 5:05 - loss: 0.143 - ETA: 5:03 - loss: 0.141 - ETA: 5:02 - loss: 0.142 - ETA: 5:00 - loss: 0.140 - ETA: 4:58 - loss: 0.139 - ETA: 4:56 - loss: 0.137 - ETA: 4:54 - loss: 0.135 - ETA: 4:52 - loss: 0.140 - ETA: 4:50 - loss: 0.141 - ETA: 4:48 - loss: 0.139 - ETA: 4:47 - loss: 0.139 - ETA: 4:45 - loss: 0.137 - ETA: 4:43 - loss: 0.136 - ETA: 4:40 - loss: 0.134 - ETA: 4:38 - loss: 0.132 - ETA: 4:36 - loss: 0.131 - ETA: 4:34 - loss: 0.129 - ETA: 4:32 - loss: 0.129 - ETA: 4:30 - loss: 0.128 - ETA: 4:28 - loss: 0.127 - ETA: 4:25 - loss: 0.125 - ETA: 4:23 - loss: 0.125 - ETA: 4:21 - loss: 0.123 - ETA: 4:19 - loss: 0.123 - ETA: 4:17 - loss: 0.122 - ETA: 4:14 - loss: 0.122 - ETA: 4:12 - loss: 0.121 - ETA: 4:10 - loss: 0.128 - ETA: 4:08 - loss: 0.127 - ETA: 4:05 - loss: 0.126 - ETA: 4:03 - loss: 0.125 - ETA: 4:00 - loss: 0.124 - ETA: 3:58 - loss: 0.123 - ETA: 3:56 - loss: 0.122 - ETA: 3:53 - loss: 0.122 - ETA: 3:51 - loss: 0.121 - ETA: 3:49 - loss: 0.121 - ETA: 3:46 - loss: 0.120 - ETA: 3:44 - loss: 0.119 - ETA: 3:41 - loss: 0.118 - ETA: 3:39 - loss: 0.118 - ETA: 3:37 - loss: 0.117 - ETA: 3:34 - loss: 0.116 - ETA: 3:32 - loss: 0.115 - ETA: 3:29 - loss: 0.114 - ETA: 3:27 - loss: 0.114 - ETA: 3:24 - loss: 0.113 - ETA: 3:22 - loss: 0.112 - ETA: 3:19 - loss: 0.112 - ETA: 3:17 - loss: 0.111 - ETA: 3:14 - loss: 0.111 - ETA: 3:12 - loss: 0.110 - ETA: 3:09 - loss: 0.109 - ETA: 3:06 - loss: 0.108 - ETA: 3:04 - loss: 0.108 - ETA: 3:01 - loss: 0.107 - ETA: 2:59 - loss: 0.106 - ETA: 2:56 - loss: 0.105 - ETA: 2:54 - loss: 0.104 - ETA: 2:51 - loss: 0.104 - ETA: 2:48 - loss: 0.103 - ETA: 2:46 - loss: 0.103 - ETA: 2:43 - loss: 0.102 - ETA: 2:40 - loss: 0.110 - ETA: 2:38 - loss: 0.110 - ETA: 2:35 - loss: 0.109 - ETA: 2:33 - loss: 0.108 - ETA: 2:30 - loss: 0.107 - ETA: 2:27 - loss: 0.108 - ETA: 2:25 - loss: 0.113 - ETA: 2:22 - loss: 0.113 - ETA: 2:19 - loss: 0.114 - ETA: 2:17 - loss: 0.113 - ETA: 2:14 - loss: 0.114 - ETA: 2:11 - loss: 0.113 - ETA: 2:09 - loss: 0.113 - ETA: 2:06 - loss: 0.113 - ETA: 2:03 - loss: 0.112 - ETA: 2:01 - loss: 0.112 - ETA: 1:58 - loss: 0.112 - ETA: 1:55 - loss: 0.111 - ETA: 1:53 - loss: 0.111 - ETA: 1:50 - loss: 0.110 - ETA: 1:47 - loss: 0.110 - ETA: 1:44 - loss: 0.109 - ETA: 1:42 - loss: 0.109 - ETA: 1:39 - loss: 0.108 - ETA: 1:36 - loss: 0.108 - ETA: 1:34 - loss: 0.108 - ETA: 1:31 - loss: 0.107 - ETA: 1:28 - loss: 0.107 - ETA: 1:25 - loss: 0.106 - ETA: 1:23 - loss: 0.106 - ETA: 1:20 - loss: 0.105 - ETA: 1:17 - loss: 0.105 - ETA: 1:15 - loss: 0.104 - ETA: 1:12 - loss: 0.104 - ETA: 1:09 - loss: 0.103 - ETA: 1:06 - loss: 0.103 - ETA: 1:04 - loss: 0.103 - ETA: 1:01 - loss: 0.102 - ETA: 58s - loss: 0.102 - ETA: 55s - loss: 0.10 - ETA: 52s - loss: 0.10 - ETA: 50s - loss: 0.10 - ETA: 47s - loss: 0.10 - ETA: 44s - loss: 0.10 - ETA: 41s - loss: 0.10 - ETA: 39s - loss: 0.09 - ETA: 36s - loss: 0.09 - ETA: 33s - loss: 0.09 - ETA: 30s - loss: 0.09 - ETA: 27s - loss: 0.09 - ETA: 25s - loss: 0.09 - ETA: 22s - loss: 0.09 - ETA: 19s - loss: 0.09 - ETA: 16s - loss: 0.09 - ETA: 14s - loss: 0.09 - ETA: 11s - loss: 0.09 - ETA: 8s - loss: 0.0951 - ETA: 5s - loss: 0.096 - ETA: 2s - loss: 0.096 - 579s 3s/step - loss: 0.0955 - val_loss: 6.6885\n",
      "Epoch 4/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186/186 [==============================] - ETA: 1:47 - loss: 0.001 - ETA: 1:47 - loss: 0.002 - ETA: 1:46 - loss: 0.002 - ETA: 1:45 - loss: 0.008 - ETA: 1:44 - loss: 0.010 - ETA: 1:44 - loss: 0.026 - ETA: 1:43 - loss: 0.024 - ETA: 1:42 - loss: 0.039 - ETA: 1:44 - loss: 0.034 - ETA: 1:43 - loss: 0.067 - ETA: 1:42 - loss: 0.067 - ETA: 1:42 - loss: 0.062 - ETA: 1:58 - loss: 0.058 - ETA: 2:24 - loss: 0.065 - ETA: 2:45 - loss: 0.067 - ETA: 3:05 - loss: 0.064 - ETA: 3:20 - loss: 0.061 - ETA: 3:35 - loss: 0.070 - ETA: 3:48 - loss: 0.068 - ETA: 4:01 - loss: 0.072 - ETA: 4:11 - loss: 0.070 - ETA: 4:21 - loss: 0.067 - ETA: 4:27 - loss: 0.064 - ETA: 4:34 - loss: 0.061 - ETA: 4:41 - loss: 0.065 - ETA: 4:46 - loss: 0.063 - ETA: 4:52 - loss: 0.064 - ETA: 4:56 - loss: 0.065 - ETA: 4:59 - loss: 0.064 - ETA: 5:03 - loss: 0.062 - ETA: 5:06 - loss: 0.061 - ETA: 5:09 - loss: 0.060 - ETA: 5:11 - loss: 0.060 - ETA: 5:13 - loss: 0.059 - ETA: 5:15 - loss: 0.057 - ETA: 5:17 - loss: 0.058 - ETA: 5:18 - loss: 0.057 - ETA: 5:19 - loss: 0.057 - ETA: 5:20 - loss: 0.055 - ETA: 5:21 - loss: 0.054 - ETA: 5:21 - loss: 0.053 - ETA: 5:22 - loss: 0.054 - ETA: 5:22 - loss: 0.053 - ETA: 5:22 - loss: 0.052 - ETA: 5:22 - loss: 0.053 - ETA: 5:21 - loss: 0.052 - ETA: 5:21 - loss: 0.052 - ETA: 5:21 - loss: 0.069 - ETA: 5:20 - loss: 0.069 - ETA: 5:19 - loss: 0.068 - ETA: 5:19 - loss: 0.068 - ETA: 5:18 - loss: 0.067 - ETA: 5:17 - loss: 0.066 - ETA: 5:16 - loss: 0.065 - ETA: 5:15 - loss: 0.064 - ETA: 5:14 - loss: 0.063 - ETA: 5:13 - loss: 0.064 - ETA: 5:12 - loss: 0.063 - ETA: 5:10 - loss: 0.067 - ETA: 5:09 - loss: 0.066 - ETA: 5:08 - loss: 0.066 - ETA: 5:06 - loss: 0.066 - ETA: 5:05 - loss: 0.066 - ETA: 5:03 - loss: 0.066 - ETA: 5:01 - loss: 0.067 - ETA: 5:00 - loss: 0.068 - ETA: 4:58 - loss: 0.068 - ETA: 4:56 - loss: 0.067 - ETA: 4:55 - loss: 0.066 - ETA: 4:53 - loss: 0.066 - ETA: 4:51 - loss: 0.066 - ETA: 4:49 - loss: 0.065 - ETA: 4:48 - loss: 0.065 - ETA: 4:46 - loss: 0.072 - ETA: 4:44 - loss: 0.071 - ETA: 4:42 - loss: 0.071 - ETA: 4:40 - loss: 0.071 - ETA: 4:38 - loss: 0.076 - ETA: 4:35 - loss: 0.075 - ETA: 4:33 - loss: 0.074 - ETA: 4:31 - loss: 0.075 - ETA: 4:29 - loss: 0.081 - ETA: 4:27 - loss: 0.080 - ETA: 4:25 - loss: 0.079 - ETA: 4:23 - loss: 0.079 - ETA: 4:20 - loss: 0.078 - ETA: 4:18 - loss: 0.080 - ETA: 4:16 - loss: 0.079 - ETA: 4:14 - loss: 0.078 - ETA: 4:11 - loss: 0.078 - ETA: 4:09 - loss: 0.077 - ETA: 4:07 - loss: 0.076 - ETA: 4:05 - loss: 0.076 - ETA: 4:02 - loss: 0.076 - ETA: 4:00 - loss: 0.075 - ETA: 3:58 - loss: 0.075 - ETA: 3:55 - loss: 0.074 - ETA: 3:53 - loss: 0.075 - ETA: 3:51 - loss: 0.075 - ETA: 3:48 - loss: 0.074 - ETA: 3:46 - loss: 0.074 - ETA: 3:43 - loss: 0.073 - ETA: 3:41 - loss: 0.072 - ETA: 3:38 - loss: 0.072 - ETA: 3:36 - loss: 0.071 - ETA: 3:33 - loss: 0.071 - ETA: 3:31 - loss: 0.070 - ETA: 3:28 - loss: 0.070 - ETA: 3:26 - loss: 0.070 - ETA: 3:23 - loss: 0.069 - ETA: 3:21 - loss: 0.069 - ETA: 3:19 - loss: 0.068 - ETA: 3:16 - loss: 0.068 - ETA: 3:14 - loss: 0.067 - ETA: 3:11 - loss: 0.067 - ETA: 3:08 - loss: 0.067 - ETA: 3:06 - loss: 0.066 - ETA: 3:03 - loss: 0.066 - ETA: 3:01 - loss: 0.067 - ETA: 2:58 - loss: 0.066 - ETA: 2:56 - loss: 0.066 - ETA: 2:53 - loss: 0.065 - ETA: 2:51 - loss: 0.065 - ETA: 2:48 - loss: 0.066 - ETA: 2:45 - loss: 0.066 - ETA: 2:43 - loss: 0.065 - ETA: 2:40 - loss: 0.065 - ETA: 2:37 - loss: 0.068 - ETA: 2:35 - loss: 0.067 - ETA: 2:32 - loss: 0.067 - ETA: 2:30 - loss: 0.066 - ETA: 2:27 - loss: 0.066 - ETA: 2:24 - loss: 0.066 - ETA: 2:22 - loss: 0.065 - ETA: 2:19 - loss: 0.065 - ETA: 2:16 - loss: 0.065 - ETA: 2:14 - loss: 0.065 - ETA: 2:11 - loss: 0.064 - ETA: 2:08 - loss: 0.064 - ETA: 2:06 - loss: 0.063 - ETA: 2:03 - loss: 0.063 - ETA: 2:01 - loss: 0.063 - ETA: 1:58 - loss: 0.063 - ETA: 1:56 - loss: 0.063 - ETA: 1:53 - loss: 0.062 - ETA: 1:50 - loss: 0.062 - ETA: 1:48 - loss: 0.062 - ETA: 1:45 - loss: 0.061 - ETA: 1:43 - loss: 0.061 - ETA: 1:40 - loss: 0.061 - ETA: 1:37 - loss: 0.061 - ETA: 1:35 - loss: 0.061 - ETA: 1:32 - loss: 0.068 - ETA: 1:29 - loss: 0.067 - ETA: 1:27 - loss: 0.067 - ETA: 1:24 - loss: 0.067 - ETA: 1:21 - loss: 0.066 - ETA: 1:18 - loss: 0.066 - ETA: 1:16 - loss: 0.065 - ETA: 1:13 - loss: 0.065 - ETA: 1:10 - loss: 0.065 - ETA: 1:07 - loss: 0.065 - ETA: 1:05 - loss: 0.065 - ETA: 1:02 - loss: 0.066 - ETA: 59s - loss: 0.066 - ETA: 56s - loss: 0.06 - ETA: 54s - loss: 0.06 - ETA: 51s - loss: 0.06 - ETA: 48s - loss: 0.06 - ETA: 45s - loss: 0.06 - ETA: 42s - loss: 0.06 - ETA: 40s - loss: 0.06 - ETA: 37s - loss: 0.06 - ETA: 34s - loss: 0.06 - ETA: 31s - loss: 0.06 - ETA: 28s - loss: 0.06 - ETA: 25s - loss: 0.06 - ETA: 23s - loss: 0.06 - ETA: 20s - loss: 0.06 - ETA: 17s - loss: 0.06 - ETA: 14s - loss: 0.06 - ETA: 11s - loss: 0.06 - ETA: 8s - loss: 0.0616 - ETA: 5s - loss: 0.061 - ETA: 2s - loss: 0.060 - 600s 3s/step - loss: 0.0606 - val_loss: 0.9154\n",
      "Epoch 5/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186/186 [==============================] - ETA: 2:12 - loss: 0.009 - ETA: 2:04 - loss: 0.054 - ETA: 2:03 - loss: 0.036 - ETA: 2:02 - loss: 0.036 - ETA: 2:03 - loss: 0.029 - ETA: 2:01 - loss: 0.024 - ETA: 2:00 - loss: 0.021 - ETA: 1:59 - loss: 0.018 - ETA: 2:01 - loss: 0.016 - ETA: 2:01 - loss: 0.021 - ETA: 2:00 - loss: 0.020 - ETA: 1:59 - loss: 0.018 - ETA: 2:16 - loss: 0.017 - ETA: 2:45 - loss: 0.016 - ETA: 3:09 - loss: 0.015 - ETA: 3:33 - loss: 0.015 - ETA: 3:51 - loss: 0.027 - ETA: 4:06 - loss: 0.027 - ETA: 4:21 - loss: 0.026 - ETA: 4:35 - loss: 0.025 - ETA: 4:45 - loss: 0.024 - ETA: 4:55 - loss: 0.023 - ETA: 5:04 - loss: 0.138 - ETA: 5:12 - loss: 0.132 - ETA: 5:20 - loss: 0.147 - ETA: 5:26 - loss: 0.146 - ETA: 5:33 - loss: 0.143 - ETA: 5:38 - loss: 0.158 - ETA: 5:42 - loss: 0.153 - ETA: 5:46 - loss: 0.149 - ETA: 5:50 - loss: 0.147 - ETA: 5:53 - loss: 0.143 - ETA: 5:56 - loss: 0.141 - ETA: 5:58 - loss: 0.138 - ETA: 5:59 - loss: 0.135 - ETA: 6:01 - loss: 0.137 - ETA: 6:02 - loss: 0.133 - ETA: 6:04 - loss: 0.131 - ETA: 6:05 - loss: 0.128 - ETA: 6:05 - loss: 0.127 - ETA: 6:06 - loss: 0.125 - ETA: 6:06 - loss: 0.145 - ETA: 6:06 - loss: 0.143 - ETA: 6:06 - loss: 0.140 - ETA: 6:06 - loss: 0.138 - ETA: 6:05 - loss: 0.137 - ETA: 6:05 - loss: 0.134 - ETA: 6:05 - loss: 0.132 - ETA: 6:04 - loss: 0.130 - ETA: 6:03 - loss: 0.129 - ETA: 6:02 - loss: 0.127 - ETA: 6:02 - loss: 0.129 - ETA: 6:00 - loss: 0.128 - ETA: 5:59 - loss: 0.126 - ETA: 5:57 - loss: 0.124 - ETA: 5:56 - loss: 0.123 - ETA: 5:55 - loss: 0.122 - ETA: 5:54 - loss: 0.121 - ETA: 5:52 - loss: 0.119 - ETA: 5:51 - loss: 0.117 - ETA: 5:49 - loss: 0.117 - ETA: 5:47 - loss: 0.118 - ETA: 5:46 - loss: 0.118 - ETA: 5:44 - loss: 0.116 - ETA: 5:42 - loss: 0.114 - ETA: 5:40 - loss: 0.113 - ETA: 5:38 - loss: 0.111 - ETA: 5:37 - loss: 0.110 - ETA: 5:35 - loss: 0.108 - ETA: 5:33 - loss: 0.107 - ETA: 5:31 - loss: 0.107 - ETA: 5:28 - loss: 0.106 - ETA: 5:26 - loss: 0.104 - ETA: 5:24 - loss: 0.103 - ETA: 5:21 - loss: 0.102 - ETA: 5:19 - loss: 0.100 - ETA: 5:17 - loss: 0.099 - ETA: 5:15 - loss: 0.098 - ETA: 5:12 - loss: 0.097 - ETA: 5:10 - loss: 0.096 - ETA: 5:08 - loss: 0.095 - ETA: 5:05 - loss: 0.094 - ETA: 5:03 - loss: 0.093 - ETA: 5:00 - loss: 0.092 - ETA: 4:58 - loss: 0.091 - ETA: 4:55 - loss: 0.093 - ETA: 4:53 - loss: 0.091 - ETA: 4:50 - loss: 0.091 - ETA: 4:48 - loss: 0.091 - ETA: 4:45 - loss: 0.090 - ETA: 4:42 - loss: 0.089 - ETA: 4:40 - loss: 0.088 - ETA: 4:37 - loss: 0.087 - ETA: 4:34 - loss: 0.086 - ETA: 4:31 - loss: 0.085 - ETA: 4:29 - loss: 0.085 - ETA: 4:26 - loss: 0.084 - ETA: 4:24 - loss: 0.083 - ETA: 4:21 - loss: 0.083 - ETA: 4:18 - loss: 0.083 - ETA: 4:15 - loss: 0.083 - ETA: 4:13 - loss: 0.084 - ETA: 4:10 - loss: 0.087 - ETA: 4:07 - loss: 0.086 - ETA: 4:04 - loss: 0.086 - ETA: 4:02 - loss: 0.085 - ETA: 3:59 - loss: 0.085 - ETA: 3:56 - loss: 0.084 - ETA: 3:53 - loss: 0.083 - ETA: 3:50 - loss: 0.083 - ETA: 3:48 - loss: 0.083 - ETA: 3:45 - loss: 0.082 - ETA: 3:42 - loss: 0.082 - ETA: 3:39 - loss: 0.081 - ETA: 3:36 - loss: 0.081 - ETA: 3:33 - loss: 0.080 - ETA: 3:30 - loss: 0.080 - ETA: 3:28 - loss: 0.080 - ETA: 3:25 - loss: 0.079 - ETA: 3:22 - loss: 0.079 - ETA: 3:19 - loss: 0.079 - ETA: 3:16 - loss: 0.078 - ETA: 3:13 - loss: 0.078 - ETA: 3:10 - loss: 0.078 - ETA: 3:07 - loss: 0.078 - ETA: 3:04 - loss: 0.077 - ETA: 3:01 - loss: 0.077 - ETA: 2:58 - loss: 0.077 - ETA: 2:55 - loss: 0.076 - ETA: 2:53 - loss: 0.076 - ETA: 2:50 - loss: 0.075 - ETA: 2:46 - loss: 0.075 - ETA: 2:43 - loss: 0.074 - ETA: 2:40 - loss: 0.074 - ETA: 2:37 - loss: 0.074 - ETA: 2:34 - loss: 0.074 - ETA: 2:31 - loss: 0.073 - ETA: 2:28 - loss: 0.073 - ETA: 2:25 - loss: 0.072 - ETA: 2:22 - loss: 0.072 - ETA: 2:19 - loss: 0.072 - ETA: 2:16 - loss: 0.072 - ETA: 2:13 - loss: 0.071 - ETA: 2:10 - loss: 0.071 - ETA: 2:07 - loss: 0.072 - ETA: 2:04 - loss: 0.071 - ETA: 2:01 - loss: 0.071 - ETA: 1:58 - loss: 0.071 - ETA: 1:55 - loss: 0.070 - ETA: 1:52 - loss: 0.070 - ETA: 1:49 - loss: 0.069 - ETA: 1:46 - loss: 0.069 - ETA: 1:43 - loss: 0.069 - ETA: 1:40 - loss: 0.070 - ETA: 1:37 - loss: 0.069 - ETA: 1:34 - loss: 0.069 - ETA: 1:30 - loss: 0.078 - ETA: 1:27 - loss: 0.078 - ETA: 1:24 - loss: 0.078 - ETA: 1:21 - loss: 0.078 - ETA: 1:18 - loss: 0.078 - ETA: 1:15 - loss: 0.078 - ETA: 1:12 - loss: 0.078 - ETA: 1:09 - loss: 0.078 - ETA: 1:06 - loss: 0.082 - ETA: 1:02 - loss: 0.082 - ETA: 59s - loss: 0.081 - ETA: 56s - loss: 0.08 - ETA: 53s - loss: 0.08 - ETA: 50s - loss: 0.08 - ETA: 47s - loss: 0.08 - ETA: 44s - loss: 0.08 - ETA: 40s - loss: 0.08 - ETA: 37s - loss: 0.08 - ETA: 34s - loss: 0.08 - ETA: 31s - loss: 0.08 - ETA: 28s - loss: 0.08 - ETA: 25s - loss: 0.07 - ETA: 22s - loss: 0.07 - ETA: 18s - loss: 0.08 - ETA: 15s - loss: 0.08 - ETA: 12s - loss: 0.08 - ETA: 9s - loss: 0.0805 - ETA: 6s - loss: 0.080 - ETA: 3s - loss: 0.079 - 646s 3s/step - loss: 0.0794 - val_loss: 6.7820\n",
      "Epoch 6/100\n",
      " 13/186 [=>............................] - ETA: 1:56 - loss: 0.044 - ETA: 1:58 - loss: 0.025 - ETA: 1:56 - loss: 0.018 - ETA: 2:02 - loss: 0.017 - ETA: 2:01 - loss: 0.016 - ETA: 2:00 - loss: 0.018 - ETA: 1:59 - loss: 0.018 - ETA: 1:58 - loss: 0.026 - ETA: 1:58 - loss: 0.026 - ETA: 1:57 - loss: 0.024 - ETA: 1:56 - loss: 0.027 - ETA: 1:55 - loss: 0.026 - ETA: 2:13 - loss: 0.0286"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-f6ced11a8b09>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_generator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m                     \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mSTEP_SIZE_VALID\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m                     epochs=num_of_epochs, callbacks=[lrReduce, chkpnt], shuffle=False) # chkpnt\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\keras\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\keras\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   2222\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[0;32m   2223\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2224\u001b[1;33m                                                class_weight=class_weight)\n\u001b[0m\u001b[0;32m   2225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2226\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\keras\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1881\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1882\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1883\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1884\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1885\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\keras\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2478\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2479\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\keras\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    903\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 905\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    906\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\keras\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1138\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1140\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1141\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\keras\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1321\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1322\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\keras\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1325\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1327\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1328\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\keras\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1312\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\keras\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[0;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1420\u001b[1;33m             status, run_metadata)\n\u001b[0m\u001b[0;32m   1421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1422\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "STEP_SIZE_TRAIN=len(df_train)//batch_size\n",
    "STEP_SIZE_VALID=len(df_val)//batch_size\n",
    "# print(STEP_SIZE_VALID)\n",
    "history = mobilenet_model.fit_generator(generator=train_generator,\n",
    "                    steps_per_epoch=STEP_SIZE_TRAIN,\n",
    "                    validation_data=val_generator,\n",
    "                    validation_steps=STEP_SIZE_VALID,\n",
    "                    epochs=num_of_epochs, callbacks=[lrReduce, chkpnt], shuffle=False) # chkpnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(test_generator)\n",
    "y_proba = mobilenet_model.predict(x)\n",
    "# mobilenet_model.predict(next())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 30ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5.113702297210693"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mobilenet_model.evaluate(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True, False,  True,  True, False,  True, False, False, False,\n",
       "        True,  True,  True, False,  True, False,  True])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_proba.argmax(axis=1)==y.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def plot_otsu_triplet(org, threshed):\n",
    "    gray = cv2.cvtColor(org, cv2.BGR2GRAY)\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    plt.subplot(131)\n",
    "    plt.imshow(org)\n",
    "    plt.subplot(132)\n",
    "    plt.imshow(gray, cmap=\"gray\")\n",
    "    plt.subplot(133)\n",
    "    plt.imshow(thr, cmap=\"gray\")\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def get_otsu_treshed_img(img, i):\n",
    "    i=10\n",
    "    assert img.max() > 1\n",
    "    x = cv2.cvtColor((img[i]).astype(np.uint8), cv2.COLOR_BGR2GRAY)re\n",
    "    x1 = cv2.GaussianBlur(x,(5,5),0)\n",
    "    ret,thr = cv2.threshold(x1,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "#     plot_otsu_triplet(org, threshed)\n",
    "    return thr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP_SIZE_TEST=len(df_test)//batch_size\n",
    "# mobilenet_model.evaluate_generator(train_generator, steps=1)\n",
    "mobilenet_model.evaluate(xxx, yyy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = mobilenet_model.predict_generator(train_generator, steps=1)\n",
    "y_pred = mobilenet_model.predict(xx)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred.argmax(axis=1), y_train.values[:, 1].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-18c767b47e3d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'y_pred' is not defined"
     ]
    }
   ],
   "source": [
    "y_test = df_test['label'][:len(y_pred)].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(y_test, y_pred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(y_pred==y_test).sum()/220"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot roc curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(y_true, y_scores, figsize=(15, 8)):\n",
    "    fpr, tpr, threshold = roc_curve(y_true, y_scores)\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.plot([0, 1], [0, 1], \"k--\")\n",
    "    plt.axes()\n",
    "\n",
    "    return fpr, tpr, threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_precision_recall_curve(y_true, y_scores, figsize=(15, 8)):\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_scores)\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.plot(precision, recall)\n",
    "#     plt.plot([0, 1], [0, 1], \"k--\")\n",
    "    plt.axes()\n",
    "    \n",
    "    return precision, recall, thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "mobilenet_model.load_weights(\"my_models/model_spec_weights_epoch36-val_loss0.006-train_loss0.004.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 1s 47ms/step\n",
      "0.0216115340590477\n",
      "16/16 [==============================] - 0s 24ms/step\n",
      "0.1286105364561081\n",
      "16/16 [==============================] - 0s 23ms/step\n",
      "0.4877246916294098\n",
      "16/16 [==============================] - 0s 24ms/step\n",
      "0.05335381254553795\n",
      "16/16 [==============================] - 0s 20ms/step\n",
      "0.092352494597435\n",
      "16/16 [==============================] - 0s 21ms/step\n",
      "0.052028391510248184\n",
      "16/16 [==============================] - 0s 23ms/step\n",
      "0.2594195008277893\n",
      "16/16 [==============================] - 0s 21ms/step\n",
      "0.18670204281806946\n",
      "16/16 [==============================] - 0s 21ms/step\n",
      "0.07823367416858673\n",
      "16/16 [==============================] - 0s 21ms/step\n",
      "0.08550003916025162\n",
      "16/16 [==============================] - 0s 22ms/step\n",
      "0.30864691734313965\n",
      "16/16 [==============================] - 0s 21ms/step\n",
      "0.08360651135444641\n",
      "16/16 [==============================] - 0s 21ms/step\n",
      "0.3477115333080292\n"
     ]
    }
   ],
   "source": [
    "y_tests = []\n",
    "y_preds = []\n",
    "for i in range(len(df_test)//batch_size):\n",
    "    x, y = next(test_generator)\n",
    "    print(mobilenet_model.evaluate(x, y))\n",
    "    y_tests.append(y.argmax(axis=1))\n",
    "    y_preds.append(mobilenet_model.predict(x)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.stack(y_tests)\n",
    "y_pred = np.stack(y_preds)\n",
    "\n",
    "y_test = y_test.reshape((-1, 1))\n",
    "y_pred = y_pred.reshape((-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\envs\\keras\\lib\\site-packages\\matplotlib\\cbook\\deprecation.py:107: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  warnings.warn(message, mplDeprecation, stacklevel=1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(roc_auc_score(y_test, y_pred))\n",
    "fpr, tpr, threshold = plot_roc_curve(y_test, y_pred, figsize=(15, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, threshold = plot_precision_recall_curve(y_test, y_pred, figsize=(15, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "STEP_SIZE_TEST=len(df_test)//batch_size\n",
    "y_proba_test = mobilenet_model.predict_generator(test_generator, steps=STEP_SIZE_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_test = to_categorical(df_test.iloc[:y_proba_test.shape[0]].label.values, num_classes=2)\n",
    "y_test = df_test['label'][:len(y_proba_test)].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,\n",
       "       0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,\n",
       "       0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,\n",
       "       0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,\n",
       "       0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n",
       "       1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,\n",
       "       1, 1, 0, 0, 0, 0, 1, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,\n",
       "       1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "       1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 0, 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_proba_test.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_test = y_test[:, 1]\n",
    "y_test = y_test.reshape((-1, 1))\n",
    "y_proba_test = y_proba_test[:, 1].reshape((-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scikitplot.metrics import plot_roc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras",
   "language": "python",
   "name": "keras"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
